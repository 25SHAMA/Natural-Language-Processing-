{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfe87ef9-c4ef-41cc-a910-c6cbe54734b3",
   "metadata": {},
   "source": [
    "## 1)Introduction to NLTK and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e61e197-4cbf-4547-8b07-5e0a2307dfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7469f162-ba76-4e65-bafb-e0596409a7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\SHAIK\n",
      "[nltk_data]     SHAMA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\SHAIK\n",
      "[nltk_data]     SHAMA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\SHAIK SHAMA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\SHAIK\n",
      "[nltk_data]     SHAMA\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(['punkt', 'punkt_tab', 'averaged_perceptron_tagger', 'wordnet'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c83a8de-0db5-4e83-9c4d-b5ed84912dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Hello Everyone. I hope you guys are doing great.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02e86c8c-59c0-4624-abe3-09fd7b324260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Everyone. I hope you guys are doing great.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43406e8a-3f52-48d1-8f70-556777c134a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Everyone', ' I hope you guys are doing great', '']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a74ea694-23cf-42ec-ab53-cd48432c6862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt.split('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0cd8dcc-321d-4f07-b6d8-32c18144f5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'Everyone.', 'I', 'hope', 'you', 'guys', 'are', 'doing', 'great.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74f780f3-1717-438e-8233-c9d356745c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4f53cba-08eb-4c8b-9193-57d9ef726515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d91d64a-b947-431b-9847-8cdbb5b4cb19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Everyone',\n",
       " '.',\n",
       " 'I',\n",
       " 'hope',\n",
       " 'you',\n",
       " 'guys',\n",
       " 'are',\n",
       " 'doing',\n",
       " 'great',\n",
       " '.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f539db3-12c3-41f3-8395-00497efd6747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Everyone.', 'I hope you guys are doing great.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6614265f-193a-48e3-82d2-5d921ddc2bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Everyone\n",
      "I\n",
      "hope\n",
      "you\n",
      "guys\n",
      "are\n",
      "doing\n",
      "great\n"
     ]
    }
   ],
   "source": [
    "for word in word_tokenize(txt):\n",
    "    if(word != '.'):\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433c8307-5cfd-433a-b747-3f5b17d88df3",
   "metadata": {},
   "source": [
    "## 2)Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb03adb5-ecac-4496-9ed7-e16b4c5c2e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\SHAIK\n",
      "[nltk_data]     SHAMA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\SHAIK\n",
      "[nltk_data]     SHAMA\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17988cdd-e772-44e8-a25b-29745c973640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "stem = PorterStemmer()\n",
    "lam = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7022ae64-98d5-4b24-a92b-c457b27a6e02",
   "metadata": {},
   "source": [
    "### 2.1)Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff5db810-d60b-4dee-adab-33a05e7e53e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change\n",
      "change\n",
      "changed\n",
      "changer\n"
     ]
    }
   ],
   "source": [
    "print(lam.lemmatize('change'))\n",
    "print(lam.lemmatize('changes'))\n",
    "print(lam.lemmatize('changed'))\n",
    "print(lam.lemmatize('changer'))##change and changes have same root word which make sense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762c4a4c-a421-454a-8d4f-f3af8f20e66e",
   "metadata": {},
   "source": [
    "### 2.2)Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b380f803-47f3-4d44-9a55-6dd4f20198c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chang\n",
      "chang\n",
      "chang\n"
     ]
    }
   ],
   "source": [
    "print(stem.stem('change'))\n",
    "print(stem.stem('changes'))\n",
    "print(stem.stem('changed'))##this gives same word which do not make sense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e845427c-8648-44a3-87c3-fb08e4c8ecf3",
   "metadata": {},
   "source": [
    "## 3)Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19abac0c-d67b-4dee-ac95-6fe79c564ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\SHAIK\n",
      "[nltk_data]     SHAMA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc3b35cd-d9a3-4f62-9474-9e73457a216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b963d1ae-f94e-44e5-b102-b5c14a1b0ab6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tyre',\n",
       " 'rreth',\n",
       " 'le',\n",
       " 'atyre',\n",
       " 'këta',\n",
       " 'megjithëse',\n",
       " 'kemi',\n",
       " 'per',\n",
       " 'ndonëse',\n",
       " 'dytë',\n",
       " 'pse',\n",
       " 'tha',\n",
       " 'aty',\n",
       " 'ndaj',\n",
       " 'ke',\n",
       " 'këtë',\n",
       " 'duhet',\n",
       " 'pa',\n",
       " 'perket',\n",
       " 'veç',\n",
       " 'ndonje',\n",
       " 'një',\n",
       " 'keshtu',\n",
       " 's',\n",
       " 'janë',\n",
       " 'jane',\n",
       " 'ti',\n",
       " 'ia',\n",
       " 'megjithese',\n",
       " 'prej',\n",
       " 'ishte',\n",
       " 'tjerë',\n",
       " 'ai',\n",
       " 'se',\n",
       " 'tillë',\n",
       " 'do',\n",
       " 'si',\n",
       " 'ja',\n",
       " 'tonë',\n",
       " 'keta',\n",
       " 'pastaj',\n",
       " 'ndersa',\n",
       " 'siç',\n",
       " 'unë',\n",
       " 'gjate',\n",
       " 'di',\n",
       " 'kësaj',\n",
       " 'cilin',\n",
       " 'kjo',\n",
       " 'dhënë',\n",
       " 'da',\n",
       " 'teper',\n",
       " 'ketij',\n",
       " 'ama',\n",
       " 'pasi',\n",
       " 'fjalë',\n",
       " 'kanë',\n",
       " 'vetem',\n",
       " 'za',\n",
       " 'd.m.th.',\n",
       " 'ose',\n",
       " 'pas',\n",
       " 'ndonjë',\n",
       " 'cila',\n",
       " 'ndodhur',\n",
       " 'dyte',\n",
       " 'ardhur',\n",
       " 'kësi',\n",
       " 'nga',\n",
       " 'vete',\n",
       " 'atij',\n",
       " 'ta',\n",
       " 'jenë',\n",
       " 'rendit',\n",
       " 'tane',\n",
       " 'keso',\n",
       " 'deri',\n",
       " 'tone',\n",
       " 'të',\n",
       " 'prandaj',\n",
       " 'bëjë',\n",
       " 'domethënë',\n",
       " 'dhe',\n",
       " 'qi',\n",
       " 'mirepo',\n",
       " 'tona',\n",
       " 'që',\n",
       " 'u',\n",
       " 'këtu',\n",
       " 'cilet',\n",
       " 'jene',\n",
       " 'tjere',\n",
       " 'gjë',\n",
       " 'së',\n",
       " 'gjatë',\n",
       " 'duhej',\n",
       " 't',\n",
       " 'dhene',\n",
       " 'thuhet',\n",
       " 'po',\n",
       " 'une',\n",
       " 'dy',\n",
       " 'cfare',\n",
       " 'ndërsa',\n",
       " 'sepse',\n",
       " 'edhe',\n",
       " 'cilen',\n",
       " 'to',\n",
       " 'meqenese',\n",
       " 'meje',\n",
       " 'tij',\n",
       " 'qene',\n",
       " 'jeni',\n",
       " 'them',\n",
       " 'përket',\n",
       " 'keto',\n",
       " 'ni',\n",
       " 'këso',\n",
       " 'asaj',\n",
       " 'ajo',\n",
       " 'sic',\n",
       " 'vetëm',\n",
       " 'ketyre',\n",
       " 'andaj',\n",
       " 'na',\n",
       " 'sa',\n",
       " 'kesaj',\n",
       " 'cili',\n",
       " 'këtyre',\n",
       " 'domethene',\n",
       " 'mirëpo',\n",
       " 'cilën',\n",
       " 'mos',\n",
       " 'madh',\n",
       " 'qenë',\n",
       " 'cilët',\n",
       " 'thënë',\n",
       " 'jemi',\n",
       " 'fjale',\n",
       " 'soje',\n",
       " 'neve',\n",
       " 'gjitha',\n",
       " 'kështu',\n",
       " 'vet',\n",
       " 'kur',\n",
       " 'ty',\n",
       " 'meqë',\n",
       " 'meqenëse',\n",
       " 'jush',\n",
       " 'ketë',\n",
       " 'para',\n",
       " 'kush',\n",
       " 'i',\n",
       " 'mua',\n",
       " 'dite',\n",
       " 'ate',\n",
       " 'për',\n",
       " 'tepër',\n",
       " 'nesh',\n",
       " 'meqe',\n",
       " 'ketu',\n",
       " 'ku',\n",
       " 'disa',\n",
       " 'ato',\n",
       " 'mbi',\n",
       " 'gje',\n",
       " 'ne',\n",
       " 'është',\n",
       " 'tille',\n",
       " 'teje',\n",
       " 'megjithate',\n",
       " 'ju',\n",
       " 'nese',\n",
       " 'saj',\n",
       " 'ashtu',\n",
       " 'më',\n",
       " 'mbasi',\n",
       " 'te',\n",
       " 'thene',\n",
       " 'jo',\n",
       " 'ditë',\n",
       " 'nuk',\n",
       " 'gjithe',\n",
       " 'shume',\n",
       " 'nje',\n",
       " 'tanë',\n",
       " 'mund',\n",
       " 'aqsa',\n",
       " 'sot',\n",
       " 'këto',\n",
       " 'tjera',\n",
       " 'tjetër',\n",
       " 'tjeter',\n",
       " 'atë',\n",
       " 'kisha',\n",
       " 'megjithatë',\n",
       " 'këtij',\n",
       " 'nëse',\n",
       " 'dimë',\n",
       " 'eshte',\n",
       " 'vazhdojmë',\n",
       " 'ka',\n",
       " 'kam',\n",
       " 'kesi',\n",
       " 'je',\n",
       " 'vazhdojme',\n",
       " 'duke',\n",
       " 'dime',\n",
       " 'kinse',\n",
       " 'por',\n",
       " 'kane',\n",
       " 'pika',\n",
       " 'keni',\n",
       " 'beje',\n",
       " 'ky',\n",
       " 'parasysh',\n",
       " 'apo',\n",
       " 'gjithë',\n",
       " 'me',\n",
       " 'ata',\n",
       " 'çfarë',\n",
       " 'jam',\n",
       " 'juve',\n",
       " 'kete',\n",
       " 'a',\n",
       " 'pra',\n",
       " 'qe',\n",
       " 'tash',\n",
       " 'në',\n",
       " 'vetë',\n",
       " 'vec',\n",
       " 'as',\n",
       " 'ndonese',\n",
       " 'tani',\n",
       " 'pak',\n",
       " 'e',\n",
       " 'shumë',\n",
       " 'إذ',\n",
       " 'إذا',\n",
       " 'إذما',\n",
       " 'إذن',\n",
       " 'أف',\n",
       " 'أقل',\n",
       " 'أكثر',\n",
       " 'ألا',\n",
       " 'إلا',\n",
       " 'التي',\n",
       " 'الذي',\n",
       " 'الذين',\n",
       " 'اللاتي',\n",
       " 'اللائي',\n",
       " 'اللتان',\n",
       " 'اللتيا',\n",
       " 'اللتين',\n",
       " 'اللذان',\n",
       " 'اللذين',\n",
       " 'اللواتي',\n",
       " 'إلى',\n",
       " 'إليك',\n",
       " 'إليكم',\n",
       " 'إليكما',\n",
       " 'إليكن',\n",
       " 'أم',\n",
       " 'أما',\n",
       " 'أما',\n",
       " 'إما',\n",
       " 'أن',\n",
       " 'إن',\n",
       " 'إنا',\n",
       " 'أنا',\n",
       " 'أنت',\n",
       " 'أنتم',\n",
       " 'أنتما',\n",
       " 'أنتن',\n",
       " 'إنما',\n",
       " 'إنه',\n",
       " 'أنى',\n",
       " 'أنى',\n",
       " 'آه',\n",
       " 'آها',\n",
       " 'أو',\n",
       " 'أولاء',\n",
       " 'أولئك',\n",
       " 'أوه',\n",
       " 'آي',\n",
       " 'أي',\n",
       " 'أيها',\n",
       " 'إي',\n",
       " 'أين',\n",
       " 'أين',\n",
       " 'أينما',\n",
       " 'إيه',\n",
       " 'بخ',\n",
       " 'بس',\n",
       " 'بعد',\n",
       " 'بعض',\n",
       " 'بك',\n",
       " 'بكم',\n",
       " 'بكم',\n",
       " 'بكما',\n",
       " 'بكن',\n",
       " 'بل',\n",
       " 'بلى',\n",
       " 'بما',\n",
       " 'بماذا',\n",
       " 'بمن',\n",
       " 'بنا',\n",
       " 'به',\n",
       " 'بها',\n",
       " 'بهم',\n",
       " 'بهما',\n",
       " 'بهن',\n",
       " 'بي',\n",
       " 'بين',\n",
       " 'بيد',\n",
       " 'تلك',\n",
       " 'تلكم',\n",
       " 'تلكما',\n",
       " 'ته',\n",
       " 'تي',\n",
       " 'تين',\n",
       " 'تينك',\n",
       " 'ثم',\n",
       " 'ثمة',\n",
       " 'حاشا',\n",
       " 'حبذا',\n",
       " 'حتى',\n",
       " 'حيث',\n",
       " 'حيثما',\n",
       " 'حين',\n",
       " 'خلا',\n",
       " 'دون',\n",
       " 'ذا',\n",
       " 'ذات',\n",
       " 'ذاك',\n",
       " 'ذان',\n",
       " 'ذانك',\n",
       " 'ذلك',\n",
       " 'ذلكم',\n",
       " 'ذلكما',\n",
       " 'ذلكن',\n",
       " 'ذه',\n",
       " 'ذو',\n",
       " 'ذوا',\n",
       " 'ذواتا',\n",
       " 'ذواتي',\n",
       " 'ذي',\n",
       " 'ذين',\n",
       " 'ذينك',\n",
       " 'ريث',\n",
       " 'سوف',\n",
       " 'سوى',\n",
       " 'شتان',\n",
       " 'عدا',\n",
       " 'عسى',\n",
       " 'عل',\n",
       " 'على',\n",
       " 'عليك',\n",
       " 'عليه',\n",
       " 'عما',\n",
       " 'عن',\n",
       " 'عند',\n",
       " 'غير',\n",
       " 'فإذا',\n",
       " 'فإن',\n",
       " 'فلا',\n",
       " 'فمن',\n",
       " 'في',\n",
       " 'فيم',\n",
       " 'فيما',\n",
       " 'فيه',\n",
       " 'فيها',\n",
       " 'قد',\n",
       " 'كأن',\n",
       " 'كأنما',\n",
       " 'كأي',\n",
       " 'كأين',\n",
       " 'كذا',\n",
       " 'كذلك',\n",
       " 'كل',\n",
       " 'كلا',\n",
       " 'كلاهما',\n",
       " 'كلتا',\n",
       " 'كلما',\n",
       " 'كليكما',\n",
       " 'كليهما',\n",
       " 'كم',\n",
       " 'كم',\n",
       " 'كما',\n",
       " 'كي',\n",
       " 'كيت',\n",
       " 'كيف',\n",
       " 'كيفما',\n",
       " 'لا',\n",
       " 'لاسيما',\n",
       " 'لدى',\n",
       " 'لست',\n",
       " 'لستم',\n",
       " 'لستما',\n",
       " 'لستن',\n",
       " 'لسن',\n",
       " 'لسنا',\n",
       " 'لعل',\n",
       " 'لك',\n",
       " 'لكم',\n",
       " 'لكما',\n",
       " 'لكن',\n",
       " 'لكنما',\n",
       " 'لكي',\n",
       " 'لكيلا',\n",
       " 'لم',\n",
       " 'لما',\n",
       " 'لن',\n",
       " 'لنا',\n",
       " 'له',\n",
       " 'لها',\n",
       " 'لهم',\n",
       " 'لهما',\n",
       " 'لهن',\n",
       " 'لو',\n",
       " 'لولا',\n",
       " 'لوما',\n",
       " 'لي',\n",
       " 'لئن',\n",
       " 'ليت',\n",
       " 'ليس',\n",
       " 'ليسا',\n",
       " 'ليست',\n",
       " 'ليستا',\n",
       " 'ليسوا',\n",
       " 'ما',\n",
       " 'ماذا',\n",
       " 'متى',\n",
       " 'مذ',\n",
       " 'مع',\n",
       " 'مما',\n",
       " 'ممن',\n",
       " 'من',\n",
       " 'منه',\n",
       " 'منها',\n",
       " 'منذ',\n",
       " 'مه',\n",
       " 'مهما',\n",
       " 'نحن',\n",
       " 'نحو',\n",
       " 'نعم',\n",
       " 'ها',\n",
       " 'هاتان',\n",
       " 'هاته',\n",
       " 'هاتي',\n",
       " 'هاتين',\n",
       " 'هاك',\n",
       " 'هاهنا',\n",
       " 'هذا',\n",
       " 'هذان',\n",
       " 'هذه',\n",
       " 'هذي',\n",
       " 'هذين',\n",
       " 'هكذا',\n",
       " 'هل',\n",
       " 'هلا',\n",
       " 'هم',\n",
       " 'هما',\n",
       " 'هن',\n",
       " 'هنا',\n",
       " 'هناك',\n",
       " 'هنالك',\n",
       " 'هو',\n",
       " 'هؤلاء',\n",
       " 'هي',\n",
       " 'هيا',\n",
       " 'هيت',\n",
       " 'هيهات',\n",
       " 'والذي',\n",
       " 'والذين',\n",
       " 'وإذ',\n",
       " 'وإذا',\n",
       " 'وإن',\n",
       " 'ولا',\n",
       " 'ولكن',\n",
       " 'ولو',\n",
       " 'وما',\n",
       " 'ومن',\n",
       " 'وهو',\n",
       " 'يا',\n",
       " 'أبٌ',\n",
       " 'أخٌ',\n",
       " 'حمٌ',\n",
       " 'فو',\n",
       " 'أنتِ',\n",
       " 'يناير',\n",
       " 'فبراير',\n",
       " 'مارس',\n",
       " 'أبريل',\n",
       " 'مايو',\n",
       " 'يونيو',\n",
       " 'يوليو',\n",
       " 'أغسطس',\n",
       " 'سبتمبر',\n",
       " 'أكتوبر',\n",
       " 'نوفمبر',\n",
       " 'ديسمبر',\n",
       " 'جانفي',\n",
       " 'فيفري',\n",
       " 'مارس',\n",
       " 'أفريل',\n",
       " 'ماي',\n",
       " 'جوان',\n",
       " 'جويلية',\n",
       " 'أوت',\n",
       " 'كانون',\n",
       " 'شباط',\n",
       " 'آذار',\n",
       " 'نيسان',\n",
       " 'أيار',\n",
       " 'حزيران',\n",
       " 'تموز',\n",
       " 'آب',\n",
       " 'أيلول',\n",
       " 'تشرين',\n",
       " 'دولار',\n",
       " 'دينار',\n",
       " 'ريال',\n",
       " 'درهم',\n",
       " 'ليرة',\n",
       " 'جنيه',\n",
       " 'قرش',\n",
       " 'مليم',\n",
       " 'فلس',\n",
       " 'هللة',\n",
       " 'سنتيم',\n",
       " 'يورو',\n",
       " 'ين',\n",
       " 'يوان',\n",
       " 'شيكل',\n",
       " 'واحد',\n",
       " 'اثنان',\n",
       " 'ثلاثة',\n",
       " 'أربعة',\n",
       " 'خمسة',\n",
       " 'ستة',\n",
       " 'سبعة',\n",
       " 'ثمانية',\n",
       " 'تسعة',\n",
       " 'عشرة',\n",
       " 'أحد',\n",
       " 'اثنا',\n",
       " 'اثني',\n",
       " 'إحدى',\n",
       " 'ثلاث',\n",
       " 'أربع',\n",
       " 'خمس',\n",
       " 'ست',\n",
       " 'سبع',\n",
       " 'ثماني',\n",
       " 'تسع',\n",
       " 'عشر',\n",
       " 'ثمان',\n",
       " 'سبت',\n",
       " 'أحد',\n",
       " 'اثنين',\n",
       " 'ثلاثاء',\n",
       " 'أربعاء',\n",
       " 'خميس',\n",
       " 'جمعة',\n",
       " 'أول',\n",
       " 'ثان',\n",
       " 'ثاني',\n",
       " 'ثالث',\n",
       " 'رابع',\n",
       " 'خامس',\n",
       " 'سادس',\n",
       " 'سابع',\n",
       " 'ثامن',\n",
       " 'تاسع',\n",
       " 'عاشر',\n",
       " 'حادي',\n",
       " 'أ',\n",
       " 'ب',\n",
       " 'ت',\n",
       " 'ث',\n",
       " 'ج',\n",
       " 'ح',\n",
       " 'خ',\n",
       " 'د',\n",
       " 'ذ',\n",
       " 'ر',\n",
       " 'ز',\n",
       " 'س',\n",
       " 'ش',\n",
       " 'ص',\n",
       " 'ض',\n",
       " 'ط',\n",
       " 'ظ',\n",
       " 'ع',\n",
       " 'غ',\n",
       " 'ف',\n",
       " 'ق',\n",
       " 'ك',\n",
       " 'ل',\n",
       " 'م',\n",
       " 'ن',\n",
       " 'ه',\n",
       " 'و',\n",
       " 'ي',\n",
       " 'ء',\n",
       " 'ى',\n",
       " 'آ',\n",
       " 'ؤ',\n",
       " 'ئ',\n",
       " 'أ',\n",
       " 'ة',\n",
       " 'ألف',\n",
       " 'باء',\n",
       " 'تاء',\n",
       " 'ثاء',\n",
       " 'جيم',\n",
       " 'حاء',\n",
       " 'خاء',\n",
       " 'دال',\n",
       " 'ذال',\n",
       " 'راء',\n",
       " 'زاي',\n",
       " 'سين',\n",
       " 'شين',\n",
       " 'صاد',\n",
       " 'ضاد',\n",
       " 'طاء',\n",
       " 'ظاء',\n",
       " 'عين',\n",
       " 'غين',\n",
       " 'فاء',\n",
       " 'قاف',\n",
       " 'كاف',\n",
       " 'لام',\n",
       " 'ميم',\n",
       " 'نون',\n",
       " 'هاء',\n",
       " 'واو',\n",
       " 'ياء',\n",
       " 'همزة',\n",
       " 'ي',\n",
       " 'نا',\n",
       " 'ك',\n",
       " 'كن',\n",
       " 'ه',\n",
       " 'إياه',\n",
       " 'إياها',\n",
       " 'إياهما',\n",
       " 'إياهم',\n",
       " 'إياهن',\n",
       " 'إياك',\n",
       " 'إياكما',\n",
       " 'إياكم',\n",
       " 'إياك',\n",
       " 'إياكن',\n",
       " 'إياي',\n",
       " 'إيانا',\n",
       " 'أولالك',\n",
       " 'تانِ',\n",
       " 'تانِك',\n",
       " 'تِه',\n",
       " 'تِي',\n",
       " 'تَيْنِ',\n",
       " 'ثمّ',\n",
       " 'ثمّة',\n",
       " 'ذانِ',\n",
       " 'ذِه',\n",
       " 'ذِي',\n",
       " 'ذَيْنِ',\n",
       " 'هَؤلاء',\n",
       " 'هَاتانِ',\n",
       " 'هَاتِه',\n",
       " 'هَاتِي',\n",
       " 'هَاتَيْنِ',\n",
       " 'هَذا',\n",
       " 'هَذانِ',\n",
       " 'هَذِه',\n",
       " 'هَذِي',\n",
       " 'هَذَيْنِ',\n",
       " 'الألى',\n",
       " 'الألاء',\n",
       " 'أل',\n",
       " 'أنّى',\n",
       " 'أيّ',\n",
       " 'ّأيّان',\n",
       " 'أنّى',\n",
       " 'أيّ',\n",
       " 'ّأيّان',\n",
       " 'ذيت',\n",
       " 'كأيّ',\n",
       " 'كأيّن',\n",
       " 'بضع',\n",
       " 'فلان',\n",
       " 'وا',\n",
       " 'آمينَ',\n",
       " 'آهِ',\n",
       " 'آهٍ',\n",
       " 'آهاً',\n",
       " 'أُفٍّ',\n",
       " 'أُفٍّ',\n",
       " 'أفٍّ',\n",
       " 'أمامك',\n",
       " 'أمامكَ',\n",
       " 'أوّهْ',\n",
       " 'إلَيْكَ',\n",
       " 'إلَيْكَ',\n",
       " 'إليكَ',\n",
       " 'إليكنّ',\n",
       " 'إيهٍ',\n",
       " 'بخٍ',\n",
       " 'بسّ',\n",
       " 'بَسْ',\n",
       " 'بطآن',\n",
       " 'بَلْهَ',\n",
       " 'حاي',\n",
       " 'حَذارِ',\n",
       " 'حيَّ',\n",
       " 'حيَّ',\n",
       " 'دونك',\n",
       " 'رويدك',\n",
       " 'سرعان',\n",
       " 'شتانَ',\n",
       " 'شَتَّانَ',\n",
       " 'صهْ',\n",
       " 'صهٍ',\n",
       " 'طاق',\n",
       " 'طَق',\n",
       " 'عَدَسْ',\n",
       " 'كِخ',\n",
       " 'مكانَك',\n",
       " 'مكانَك',\n",
       " 'مكانَك',\n",
       " 'مكانكم',\n",
       " 'مكانكما',\n",
       " 'مكانكنّ',\n",
       " 'نَخْ',\n",
       " 'هاكَ',\n",
       " 'هَجْ',\n",
       " 'هلم',\n",
       " 'هيّا',\n",
       " 'هَيْهات',\n",
       " 'وا',\n",
       " 'واهاً',\n",
       " 'وراءَك',\n",
       " 'وُشْكَانَ',\n",
       " 'وَيْ',\n",
       " 'يفعلان',\n",
       " 'تفعلان',\n",
       " 'يفعلون',\n",
       " 'تفعلون',\n",
       " 'تفعلين',\n",
       " 'اتخذ',\n",
       " 'ألفى',\n",
       " 'تخذ',\n",
       " 'ترك',\n",
       " 'تعلَّم',\n",
       " 'جعل',\n",
       " 'حجا',\n",
       " 'حبيب',\n",
       " 'خال',\n",
       " 'حسب',\n",
       " 'خال',\n",
       " 'درى',\n",
       " 'رأى',\n",
       " 'زعم',\n",
       " 'صبر',\n",
       " 'ظنَّ',\n",
       " 'عدَّ',\n",
       " 'علم',\n",
       " 'غادر',\n",
       " 'ذهب',\n",
       " 'وجد',\n",
       " 'ورد',\n",
       " 'وهب',\n",
       " 'أسكن',\n",
       " 'أطعم',\n",
       " 'أعطى',\n",
       " 'رزق',\n",
       " 'زود',\n",
       " 'سقى',\n",
       " 'كسا',\n",
       " 'أخبر',\n",
       " 'أرى',\n",
       " 'أعلم',\n",
       " 'أنبأ',\n",
       " 'حدَث',\n",
       " 'خبَّر',\n",
       " 'نبَّا',\n",
       " 'أفعل به',\n",
       " 'ما أفعله',\n",
       " 'بئس',\n",
       " 'ساء',\n",
       " 'طالما',\n",
       " 'قلما',\n",
       " 'لات',\n",
       " 'لكنَّ',\n",
       " 'ءَ',\n",
       " 'أجل',\n",
       " 'إذاً',\n",
       " 'أمّا',\n",
       " 'إمّا',\n",
       " 'إنَّ',\n",
       " 'أنًّ',\n",
       " 'أى',\n",
       " 'إى',\n",
       " 'أيا',\n",
       " 'ب',\n",
       " 'ثمَّ',\n",
       " 'جلل',\n",
       " 'جير',\n",
       " 'رُبَّ',\n",
       " 'س',\n",
       " 'علًّ',\n",
       " 'ف',\n",
       " 'كأنّ',\n",
       " 'كلَّا',\n",
       " 'كى',\n",
       " 'ل',\n",
       " 'لات',\n",
       " 'لعلَّ',\n",
       " 'لكنَّ',\n",
       " 'لكنَّ',\n",
       " 'م',\n",
       " 'نَّ',\n",
       " 'هلّا',\n",
       " 'وا',\n",
       " 'أل',\n",
       " 'إلّا',\n",
       " 'ت',\n",
       " 'ك',\n",
       " 'لمّا',\n",
       " 'ن',\n",
       " 'ه',\n",
       " 'و',\n",
       " 'ا',\n",
       " 'ي',\n",
       " 'تجاه',\n",
       " 'تلقاء',\n",
       " 'جميع',\n",
       " 'حسب',\n",
       " 'سبحان',\n",
       " 'شبه',\n",
       " 'لعمر',\n",
       " 'مثل',\n",
       " 'معاذ',\n",
       " 'أبو',\n",
       " 'أخو',\n",
       " 'حمو',\n",
       " 'فو',\n",
       " 'مئة',\n",
       " 'مئتان',\n",
       " 'ثلاثمئة',\n",
       " 'أربعمئة',\n",
       " 'خمسمئة',\n",
       " 'ستمئة',\n",
       " 'سبعمئة',\n",
       " 'ثمنمئة',\n",
       " 'تسعمئة',\n",
       " 'مائة',\n",
       " 'ثلاثمائة',\n",
       " 'أربعمائة',\n",
       " 'خمسمائة',\n",
       " 'ستمائة',\n",
       " 'سبعمائة',\n",
       " 'ثمانمئة',\n",
       " 'تسعمائة',\n",
       " 'عشرون',\n",
       " 'ثلاثون',\n",
       " 'اربعون',\n",
       " 'خمسون',\n",
       " 'ستون',\n",
       " 'سبعون',\n",
       " 'ثمانون',\n",
       " 'تسعون',\n",
       " 'عشرين',\n",
       " 'ثلاثين',\n",
       " 'اربعين',\n",
       " 'خمسين',\n",
       " 'ستين',\n",
       " 'سبعين',\n",
       " 'ثمانين',\n",
       " 'تسعين',\n",
       " 'بضع',\n",
       " 'نيف',\n",
       " 'أجمع',\n",
       " 'جميع',\n",
       " 'عامة',\n",
       " 'عين',\n",
       " 'نفس',\n",
       " 'لا سيما',\n",
       " 'أصلا',\n",
       " 'أهلا',\n",
       " 'أيضا',\n",
       " 'بؤسا',\n",
       " 'بعدا',\n",
       " 'بغتة',\n",
       " 'تعسا',\n",
       " 'حقا',\n",
       " 'حمدا',\n",
       " 'خلافا',\n",
       " 'خاصة',\n",
       " 'دواليك',\n",
       " 'سحقا',\n",
       " 'سرا',\n",
       " 'سمعا',\n",
       " 'صبرا',\n",
       " 'صدقا',\n",
       " 'صراحة',\n",
       " 'طرا',\n",
       " 'عجبا',\n",
       " 'عيانا',\n",
       " 'غالبا',\n",
       " 'فرادى',\n",
       " 'فضلا',\n",
       " 'قاطبة',\n",
       " 'كثيرا',\n",
       " 'لبيك',\n",
       " 'معاذ',\n",
       " 'أبدا',\n",
       " 'إزاء',\n",
       " 'أصلا',\n",
       " 'الآن',\n",
       " 'أمد',\n",
       " 'أمس',\n",
       " 'آنفا',\n",
       " 'آناء',\n",
       " 'أنّى',\n",
       " 'أول',\n",
       " 'أيّان',\n",
       " 'تارة',\n",
       " 'ثمّ',\n",
       " 'ثمّة',\n",
       " 'حقا',\n",
       " 'صباح',\n",
       " 'مساء',\n",
       " 'ضحوة',\n",
       " 'عوض',\n",
       " 'غدا',\n",
       " 'غداة',\n",
       " 'قطّ',\n",
       " 'كلّما',\n",
       " 'لدن',\n",
       " 'لمّا',\n",
       " 'مرّة',\n",
       " 'قبل',\n",
       " 'خلف',\n",
       " 'أمام',\n",
       " 'فوق',\n",
       " 'تحت',\n",
       " 'يمين',\n",
       " 'شمال',\n",
       " 'ارتدّ',\n",
       " 'استحال',\n",
       " 'أصبح',\n",
       " 'أضحى',\n",
       " 'آض',\n",
       " 'أمسى',\n",
       " 'انقلب',\n",
       " 'بات',\n",
       " 'تبدّل',\n",
       " 'تحوّل',\n",
       " 'حار',\n",
       " 'رجع',\n",
       " 'راح',\n",
       " 'صار',\n",
       " 'ظلّ',\n",
       " 'عاد',\n",
       " 'غدا',\n",
       " 'كان',\n",
       " 'ما انفك',\n",
       " 'ما برح',\n",
       " 'مادام',\n",
       " 'مازال',\n",
       " 'مافتئ',\n",
       " 'ابتدأ',\n",
       " 'أخذ',\n",
       " 'اخلولق',\n",
       " 'أقبل',\n",
       " 'انبرى',\n",
       " 'أنشأ',\n",
       " 'أوشك',\n",
       " 'جعل',\n",
       " 'حرى',\n",
       " 'شرع',\n",
       " 'طفق',\n",
       " 'علق',\n",
       " 'قام',\n",
       " 'كرب',\n",
       " 'كاد',\n",
       " 'هبّa',\n",
       " 'ad',\n",
       " 'altı',\n",
       " 'altmış',\n",
       " 'amma',\n",
       " 'arasında',\n",
       " 'artıq',\n",
       " 'ay',\n",
       " 'az',\n",
       " 'bax',\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words()#this gives stop words from all the languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97c1c08f-317e-4c30-8818-e2095d9732f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword = stopwords.words('english')#specifically in english\n",
    "stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6bcad33-a8d2-4e02-a449-6e5b76eb2539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ad8904a-0916-466a-96de-3c88e46d02ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'This is not a good time to talk.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cce8f51b-774b-4f53-aa90-f9a4a4678a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is not a good time to talk.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "18796334-524c-4fa8-b233-a82abd8820f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = word_tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86fe52a7-07be-46da-be0f-d44ba23176ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'not', 'a', 'good', 'time', 'to', 'talk', '.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e35bb361-1c9e-4eac-8896-ba8172982011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "not\n",
      "a\n",
      "good\n",
      "time\n",
      "to\n",
      "talk\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for word in txt:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31c98568-faa4-4365-a820-48296267caea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This False\n",
      "is True\n",
      "not True\n",
      "a True\n",
      "good False\n",
      "time False\n",
      "to True\n",
      "talk False\n",
      ". False\n"
     ]
    }
   ],
   "source": [
    "for word in txt:\n",
    "    print(word , word in stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "44b09bbe-86f4-4edc-b2aa-39663398ebed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "not\n",
      "a\n",
      "to\n"
     ]
    }
   ],
   "source": [
    "for word in txt:\n",
    "    if (word.lower() in stopword):\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e79365ac-789c-41ea-be97-8e85e6f74967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "time\n",
      "talk\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for word in txt:\n",
    "    if (word.lower() not in stopword):\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c2d32b3d-1e44-435d-a04d-dc664f5b4171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "time\n",
      "talk\n"
     ]
    }
   ],
   "source": [
    "for word in txt:\n",
    "    if ((word.lower() not in stopword) and (len(word) > 2)):\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c061cf5d-073b-4fe7-9ee5-0c22a2305ae1",
   "metadata": {},
   "source": [
    "## 4)Corpus and Vocabulary\n",
    "       corpus - a huge text (multiple paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f79a6b1e-6fcd-4e75-bca9-58a83405b497",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"India is a beautiful land with a variety of wildlife and rich cultural diversity. The Bengal Tiger is considered the national animal of India. India celebrates its Independence Day on 15th August every year. It is observed to commemorate the freedom of India from the British. The tri-coloured national flag is called Tiranga, designed with saffron, white and green with the Ashok Chakra in navy blue at the centre of the flag.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "705a0fab-a661-4133-ad2f-9b079c38eb5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'India is a beautiful land with a variety of wildlife and rich cultural diversity. The Bengal Tiger is considered the national animal of India. India celebrates its Independence Day on 15th August every year. It is observed to commemorate the freedom of India from the British. The tri-coloured national flag is called Tiranga, designed with saffron, white and green with the Ashok Chakra in navy blue at the centre of the flag.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da225ced-6c95-4ce4-92af-1b39ee6caa7f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['India',\n",
       " 'is',\n",
       " 'a',\n",
       " 'beautiful',\n",
       " 'land',\n",
       " 'with',\n",
       " 'a',\n",
       " 'variety',\n",
       " 'of',\n",
       " 'wildlife',\n",
       " 'and',\n",
       " 'rich',\n",
       " 'cultural',\n",
       " 'diversity',\n",
       " '.',\n",
       " 'The',\n",
       " 'Bengal',\n",
       " 'Tiger',\n",
       " 'is',\n",
       " 'considered',\n",
       " 'the',\n",
       " 'national',\n",
       " 'animal',\n",
       " 'of',\n",
       " 'India',\n",
       " '.',\n",
       " 'India',\n",
       " 'celebrates',\n",
       " 'its',\n",
       " 'Independence',\n",
       " 'Day',\n",
       " 'on',\n",
       " '15th',\n",
       " 'August',\n",
       " 'every',\n",
       " 'year',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'observed',\n",
       " 'to',\n",
       " 'commemorate',\n",
       " 'the',\n",
       " 'freedom',\n",
       " 'of',\n",
       " 'India',\n",
       " 'from',\n",
       " 'the',\n",
       " 'British',\n",
       " '.',\n",
       " 'The',\n",
       " 'tri-coloured',\n",
       " 'national',\n",
       " 'flag',\n",
       " 'is',\n",
       " 'called',\n",
       " 'Tiranga',\n",
       " ',',\n",
       " 'designed',\n",
       " 'with',\n",
       " 'saffron',\n",
       " ',',\n",
       " 'white',\n",
       " 'and',\n",
       " 'green',\n",
       " 'with',\n",
       " 'the',\n",
       " 'Ashok',\n",
       " 'Chakra',\n",
       " 'in',\n",
       " 'navy',\n",
       " 'blue',\n",
       " 'at',\n",
       " 'the',\n",
       " 'centre',\n",
       " 'of',\n",
       " 'the',\n",
       " 'flag',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2d7c40f-1832-47d1-add5-5f52a696ef4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The unique words we want which is size of vocabulary\n",
    "#The more vocab size we are having it is difficult our model to train,lesser the vocab size it is easier\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0cdd35-23e7-4a80-838f-4d94d79b948f",
   "metadata": {},
   "source": [
    "#### Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b84bacd-ff04-4110-bdf6-a672876b5e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India\n",
      "beautiful\n",
      "land\n",
      "variety\n",
      "wildlife\n",
      "rich\n",
      "cultural\n",
      "diversity\n",
      "Bengal\n",
      "Tiger\n",
      "considered\n",
      "national\n",
      "animal\n",
      "India\n",
      "India\n",
      "celebrates\n",
      "Independence\n",
      "Day\n",
      "15th\n",
      "August\n",
      "every\n",
      "year\n",
      "observed\n",
      "commemorate\n",
      "freedom\n",
      "India\n",
      "British\n",
      "tri-coloured\n",
      "national\n",
      "flag\n",
      "called\n",
      "Tiranga\n",
      "designed\n",
      "saffron\n",
      "white\n",
      "green\n",
      "Ashok\n",
      "Chakra\n",
      "navy\n",
      "blue\n",
      "centre\n",
      "flag\n"
     ]
    }
   ],
   "source": [
    "for word in word_tokenize(corpus):\n",
    "    if (word.lower() not in stopwords.words('english') and len(word) >= 2):\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6478ef46-320b-4237-89d9-3da0b03478c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [] \n",
    "\n",
    "for word in word_tokenize(corpus):\n",
    "    if (word.lower() not in stopwords.words('english') and len(word) >= 2):\n",
    "        words.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f205f780-b9a5-4a99-93d6-6b01066d0620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['india',\n",
       " 'beautiful',\n",
       " 'land',\n",
       " 'variety',\n",
       " 'wildlife',\n",
       " 'rich',\n",
       " 'cultural',\n",
       " 'diversity',\n",
       " 'bengal',\n",
       " 'tiger',\n",
       " 'considered',\n",
       " 'national',\n",
       " 'animal',\n",
       " 'india',\n",
       " 'india',\n",
       " 'celebrates',\n",
       " 'independence',\n",
       " 'day',\n",
       " '15th',\n",
       " 'august',\n",
       " 'every',\n",
       " 'year',\n",
       " 'observed',\n",
       " 'commemorate',\n",
       " 'freedom',\n",
       " 'india',\n",
       " 'british',\n",
       " 'tri-coloured',\n",
       " 'national',\n",
       " 'flag',\n",
       " 'called',\n",
       " 'tiranga',\n",
       " 'designed',\n",
       " 'saffron',\n",
       " 'white',\n",
       " 'green',\n",
       " 'ashok',\n",
       " 'chakra',\n",
       " 'navy',\n",
       " 'blue',\n",
       " 'centre',\n",
       " 'flag']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de67be44-40e8-43f5-af5b-9dc2b2afb555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'15th',\n",
       " 'animal',\n",
       " 'ashok',\n",
       " 'august',\n",
       " 'beautiful',\n",
       " 'bengal',\n",
       " 'blue',\n",
       " 'british',\n",
       " 'called',\n",
       " 'celebrates',\n",
       " 'centre',\n",
       " 'chakra',\n",
       " 'commemorate',\n",
       " 'considered',\n",
       " 'cultural',\n",
       " 'day',\n",
       " 'designed',\n",
       " 'diversity',\n",
       " 'every',\n",
       " 'flag',\n",
       " 'freedom',\n",
       " 'green',\n",
       " 'independence',\n",
       " 'india',\n",
       " 'land',\n",
       " 'national',\n",
       " 'navy',\n",
       " 'observed',\n",
       " 'rich',\n",
       " 'saffron',\n",
       " 'tiger',\n",
       " 'tiranga',\n",
       " 'tri-coloured',\n",
       " 'variety',\n",
       " 'white',\n",
       " 'wildlife',\n",
       " 'year'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to get unique words \n",
    "set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b07ac50-6dc3-410e-b420-028565516bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2627b30f-3936-44a0-b1ef-a1ab494a4f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a16a261c-454e-4962-8d17-d064d61e10fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_tokenize(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9947784f-5549-41ff-89b4-6e16d86246ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef9094fd-287d-4172-b9d2-51f7850d7bd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['land',\n",
       " 'cultural',\n",
       " 'day',\n",
       " 'wildlife',\n",
       " 'animal',\n",
       " 'centre',\n",
       " 'british',\n",
       " 'designed',\n",
       " 'commemorate',\n",
       " 'diversity',\n",
       " 'called',\n",
       " 'considered',\n",
       " 'white',\n",
       " 'variety',\n",
       " 'india',\n",
       " 'celebrates',\n",
       " 'blue',\n",
       " 'rich',\n",
       " 'observed',\n",
       " 'tri-coloured',\n",
       " 'independence',\n",
       " 'saffron',\n",
       " 'beautiful',\n",
       " 'freedom',\n",
       " 'tiranga',\n",
       " 'green',\n",
       " 'bengal',\n",
       " 'national',\n",
       " '15th',\n",
       " 'flag',\n",
       " 'navy',\n",
       " 'tiger',\n",
       " 'chakra',\n",
       " 'ashok',\n",
       " 'august',\n",
       " 'year',\n",
       " 'every']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2039cfdb-a684-473c-820f-25b54f0ae4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['India is a beautiful land with a variety of wildlife and rich cultural diversity.',\n",
       " 'The Bengal Tiger is considered the national animal of India.',\n",
       " 'India celebrates its Independence Day on 15th August every year.',\n",
       " 'It is observed to commemorate the freedom of India from the British.',\n",
       " 'The tri-coloured national flag is called Tiranga, designed with saffron, white and green with the Ashok Chakra in navy blue at the centre of the flag.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ead3d0a-40ff-40e6-8b63-6d175d75ff7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India is a beautiful land with a variety of wildlife and rich cultural diversity.\n",
      "The Bengal Tiger is considered the national animal of India.\n",
      "India celebrates its Independence Day on 15th August every year.\n",
      "It is observed to commemorate the freedom of India from the British.\n",
      "The tri-coloured national flag is called Tiranga, designed with saffron, white and green with the Ashok Chakra in navy blue at the centre of the flag.\n"
     ]
    }
   ],
   "source": [
    "for sent in sent_tokenize(corpus):\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0fbfbdd4-4ff1-46fa-928f-804117937a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Encoding Tecnhique we are going to apply which is\n",
    "\n",
    "# this is a car\n",
    "#this = 1\n",
    "#is = 2\n",
    "#a = 3\n",
    "#car = 4\n",
    "\n",
    "# 1 2 3 4(this is a car)\n",
    "# 2 1 3 4(is this a car)# we will see how to apply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c28634a-4712-4931-b2b9-6ec3d24c226b",
   "metadata": {},
   "source": [
    "## 5)Vocabulary With Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f0d0b5c-1d87-4525-ad43-40e308186db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (2.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (4.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (2.1.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\shaik shama\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31afb63c-2665-439d-94d1-5e924ab54196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "019ac0aa-96a7-4476-bd16-195ac612c164",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43546f69-836a-49b7-bdf2-99d32a079427",
   "metadata": {},
   "outputs": [],
   "source": [
    "corp = ['Coffee is hot' , 'Water is Cold']\n",
    "\n",
    "tok.fit_on_texts(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed7525d3-8e8a-493d-a6b6-7e426b57f492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is': 1, 'coffee': 2, 'hot': 3, 'water': 4, 'cold': 5}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8c90e5f-1bee-4d35-89ac-50811d207196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 1, 3], [4, 1, 5]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.texts_to_sequences(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a07a12b-3472-4327-9aa8-1e530db68c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 1, 5], [2, 1, 3]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if we add a new word in the corpus we will not get the sequence number for that now let us see that\n",
    "tok.texts_to_sequences(['water is cold', 'black coffee is hot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4302ce7-c3bc-4819-9c05-1be72e1be052",
   "metadata": {},
   "source": [
    "#### Adding OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac22d069-d99d-49e9-a34b-9050735ef484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'black': 1, 'is': 2, 'coffee': 3, 'hot': 4, 'water': 5, 'cold': 6}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[5, 2, 6], [1, 3, 2, 4]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To add the sequence number to black let us see what we will do now\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tok = Tokenizer(oov_token = 'black')\n",
    "\n",
    "corp = ['Coffee is hot' , 'Water is Cold']\n",
    "tok.fit_on_texts(corp)\n",
    "print(tok.word_index)\n",
    "\n",
    "tok.texts_to_sequences(['water is cold', 'black coffee is hot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2fb003-e04a-451b-a7ec-498577110c0a",
   "metadata": {},
   "source": [
    "#### Limiting the number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8086302-e5eb-46f0-b773-70035aeb73b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 1, 'coffee': 2, 'hot': 3, 'water': 4, 'cold': 5}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1], [2, 1]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tok = Tokenizer(num_words = 3)\n",
    "\n",
    "corp = ['Coffee is hot' , 'Water is Cold']\n",
    "tok.fit_on_texts(corp)\n",
    "print(tok.word_index)\n",
    "\n",
    "tok.texts_to_sequences(['water is cold', 'black coffee is hot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4b75c20-cbd7-4d60-9f06-d5552fb888c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 1, 'coffee': 2, 'hot': 3, 'water': 4, 'cold': 5}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[4, 1, 5], [2, 1, 3]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tok = Tokenizer(num_words = 6)\n",
    "\n",
    "corp = ['Coffee is hot' , 'Water is Cold']\n",
    "tok.fit_on_texts(corp)\n",
    "print(tok.word_index)\n",
    "\n",
    "tok.texts_to_sequences(['water is cold', 'black coffee is hot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ea92f1-363f-43bc-b9f7-985c4cc35736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
